import torch
import numpy as np
import torchvision.transforms as transforms
from torchvision import datasets
from torch.utils.data import DataLoader, Subset
from collections import Counter
import os


def get_cifar10_loaders(num_clients: int, dirichlet_alpha: float = 0.5,
                        data_root: str = './data', batch_size_val: int = 256,
                        num_workers: int = 8, samples_per_client: int = 2000):
    """
    CIFAR-10 ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³ , ê° í´ë¼ì´ì–¸íŠ¸(ìœ„ì„±)ì—ê²Œ
    Dirichlet ë¶„í¬ ê¸°ë°˜ Non-IID ë°ì´í„°ë¥¼ **ë…ë¦½ ìƒ˜í”Œë§**í•©ë‹ˆë‹¤.

    ê¸°ì¡´ ë°©ì‹(split)ê³¼ì˜ ì°¨ì´:
      - split: 50,000ì¥ì„ Në“±ë¶„ â†’ ìœ„ì„±ë‹¹ ~210ì¥ (N=238)
      - sample: ìœ„ì„±ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ samples_per_clientì¥ì„ Dirichlet ë¹„ìœ¨ë¡œ ìƒ˜í”Œë§
               â†’ ìœ„ì„± ê°„ ë°ì´í„° ì¤‘ë³µ í—ˆìš© (ìœ„ì„±ë“¤ì´ ìœ ì‚¬ ì§€ì—­ ì´¬ì˜í•˜ëŠ” í˜„ì‹¤ ë°˜ì˜)

    Args:
        num_clients: í´ë¼ì´ì–¸íŠ¸(ìœ„ì„±) ìˆ˜
        dirichlet_alpha: Non-IID ê°•ë„ (ì‘ì„ìˆ˜ë¡ í¸í–¥ â†‘, 0.5 = moderate)
        data_root: ë°ì´í„° ì €ì¥ ê²½ë¡œ
        batch_size_val: ê²€ì¦ ë°°ì¹˜ í¬ê¸°
        num_workers: DataLoader ì›Œì»¤ ìˆ˜
        samples_per_client: ìœ„ì„±ë‹¹ í•™ìŠµ ë°ì´í„° ìˆ˜ (ê¸°ë³¸ 2000)
    """

    # 1. CIFAR-10 ì „ìš© ì •ê·œí™” ê°’ (Mean, Std)
    CIFAR_MEAN = (0.4914, 0.4822, 0.4465)
    CIFAR_STD  = (0.2023, 0.1994, 0.2010)

    # 2. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì •ì˜
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(CIFAR_MEAN, CIFAR_STD),
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(CIFAR_MEAN, CIFAR_STD),
    ])

    print(f"ğŸ“¥ [Data] CIFAR-10 ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘... (Root: {data_root})")

    # 3. ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ
    train_dataset = datasets.CIFAR10(
        root=data_root, train=True, download=True, transform=transform_train
    )
    test_dataset = datasets.CIFAR10(
        root=data_root, train=False, download=True, transform=transform_test
    )

    # 4. í´ë˜ìŠ¤ë³„ ì¸ë±ìŠ¤ ì‚¬ì „ êµ¬ì¶•
    targets = np.array(train_dataset.targets)
    num_classes = 10
    class_indices = {k: np.where(targets == k)[0] for k in range(num_classes)}

    print(
        f"âš–ï¸ [Data] Dirichlet(Î±={dirichlet_alpha}) ë…ë¦½ ìƒ˜í”Œë§: "
        f"{num_clients}ê°œ ìœ„ì„± Ã— {samples_per_client}ì¥/ìœ„ì„±"
    )

    # 5. ìœ„ì„±ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ Dirichlet ìƒ˜í”Œë§
    client_subsets = []
    total_data_count = 0

    for i in range(num_clients):
        # (a) Dirichlet ë¶„í¬ë¡œ ì´ ìœ„ì„±ì˜ í´ë˜ìŠ¤ ë¹„ìœ¨ ìƒì„±
        class_probs = np.random.dirichlet(np.repeat(dirichlet_alpha, num_classes))

        # (b) ë¹„ìœ¨ì— ë”°ë¼ í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ ê²°ì •
        class_counts = np.round(class_probs * samples_per_client).astype(int)

        # ë°˜ì˜¬ë¦¼ ì˜¤ì°¨ ë³´ì •: ì´í•©ì´ samples_per_clientì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
        diff = samples_per_client - class_counts.sum()
        if diff != 0:
            # ê°€ì¥ ë¹„ìœ¨ì´ í° í´ë˜ìŠ¤ì—ì„œ ì¡°ì •
            max_class = np.argmax(class_counts)
            class_counts[max_class] += diff

        # ê° í´ë˜ìŠ¤ì—ì„œ ìµœì†Œ 1ê°œëŠ” ë³´ì¥í•˜ì§€ ì•ŠìŒ (Non-IID íŠ¹ì„± ìœ ì§€)
        # ë‹¨, ìŒìˆ˜ ë°©ì§€
        class_counts = np.maximum(class_counts, 0)

        # (c) í´ë˜ìŠ¤ë³„ë¡œ ì¤‘ë³µ í—ˆìš© ëœë¤ ìƒ˜í”Œë§
        selected_indices = []
        for k in range(num_classes):
            n_samples = class_counts[k]
            if n_samples == 0:
                continue
            pool = class_indices[k]
            # replace=True: ì¤‘ë³µ í—ˆìš© (ìœ„ì„± ê°„ + ìœ„ì„± ë‚´ í´ë˜ìŠ¤ ë‚´)
            # ìœ„ì„± ë‚´ ì¤‘ë³µì€ augmentationì´ ë‹¤ë¥´ë¯€ë¡œ ì‹¤ì§ˆì ìœ¼ë¡œ ë‹¤ë¥¸ ìƒ˜í”Œ
            sampled = np.random.choice(pool, size=n_samples, replace=True)
            selected_indices.extend(sampled)

        np.random.shuffle(selected_indices)
        subset = Subset(train_dataset, selected_indices)
        client_subsets.append(subset)
        total_data_count += len(selected_indices)

    avg_data_count = total_data_count / num_clients

    # 6. Global Validation Loader ìƒì„±
    val_loader = DataLoader(
        test_dataset,
        batch_size=batch_size_val,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )

    # (ë””ë²„ê¹…) ë¶„í•  ê²°ê³¼ ìš”ì•½ ì¶œë ¥ (ì²« 5ê°œ ìœ„ì„±)
    print(
        f"ğŸ“Š ìƒ˜í”Œë§ ì™„ë£Œ: ìœ„ì„±ë‹¹ {samples_per_client}ì¥ "
        f"(ì´ {total_data_count:,}ì¥, ì¤‘ë³µ í—ˆìš©)"
    )
    for i in range(min(5, num_clients)):
        indices = [client_subsets[i].indices[j]
                   for j in range(len(client_subsets[i]))]
        labels = [targets[idx] for idx in indices]
        counts = Counter(labels)
        dist_str = ' '.join(f"{k}:{v}" for k, v in sorted(counts.items()))
        print(f"  - SAT_{i}: {len(indices)} samples [{dist_str}]")

    return avg_data_count, client_subsets, val_loader, train_dataset.classes